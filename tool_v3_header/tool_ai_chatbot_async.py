import anthropic
from openai import AsyncOpenAI
from dotenv import load_dotenv
import os
import pandas as pd
import re
import tiktoken
from pinecone import Pinecone
from datetime import datetime
import asyncio
from tool_async.tool_postgreSQL import AsyncSessionLocal, Conversation
from sqlalchemy import insert

load_dotenv()  # è®€å– .env æª”æ¡ˆ
# clientGPT = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
# clientCLAUDE = anthropic.Anthropic(api_key=os.getenv("CLAUDE_API_KEY"))
# clientXAI = anthropic.Anthropic(api_key=os.getenv("XAI_API_KEY"), base_url="https://api.x.ai/",)
# pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))

global_reference_id = 1

class AIChatbot:
    def __init__(self, bot_name, ai_model="gpt-4o"):
        """
        åˆå§‹åŒ–ä¸€å€‹ AI æ©Ÿå™¨äºº
        :param bot_name: æ©Ÿå™¨äººåç¨±
        :param ai_model: AI æ¨¡å‹ (é è¨­ç‚º gpt-4oï¼Œå¯é¸ claude-3-5-sonnet ç­‰)
        :param training_data: é è¼‰çš„è¨“ç·´æ•¸æ“š
        """
        load_dotenv()
        self.clientGPT = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.clientCLAUDE = anthropic.AsyncAnthropic(api_key=os.getenv("CLAUDE_API_KEY"))
        self.pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))
        self.bot_name = bot_name
        self.ai_model = ai_model
        self.messages_history = []  # ç¨ç«‹çš„å°è©±æ­·å²
        self.messages_history_ai = []
        self.messages_history_ui = []
        self.message_folder_path = f"./database/{self.bot_name}/"
        ensure_directory_exists(self.message_folder_path)
        self.training_message = None
        self.print_prompt = True
    def get_messages_history(self):
        """ ç²å–å°è©±ç´€éŒ„ """
        return self.messages_history
    def read_previous_messages(self, message_file_path=None, messages_history=None):
        messages = []
        if message_file_path:
            if os.path.exists(message_file_path):
                # è®€å– CSV æª”æ¡ˆä¸¦è½‰æ›æˆå°æ‡‰çš„æ ¼å¼
                messages_history_df = pd.read_csv(message_file_path)
                messages = messages_history_df.to_dict(orient='records')
                print(f"è®€å–ä¹‹å‰çš„æ­·å²è¨Šæ¯ï¼š\n{messages}")
            # æª¢æŸ¥æœ€å¾Œä¸€ç­†è³‡æ–™çš„ roleï¼Œå¦‚æœæ˜¯ 'user'ï¼Œå‰‡åˆªé™¤
            if messages and messages[-1]["role"] == "user":
                messages.pop()
            self.messages_history_ai.extend(messages)
            self.messages_history_ui.extend(messages)
        if messages_history:
            self.messages_history_ai.extend(messages_history)
            self.messages_history_ui.extend(messages_history)
    def read_training_messages(self, message_file_path):
        self.training_message = get_filepath_to_messages(message_file_path)
    async def send_message(self, message):
        """
        å‘ AI é€å‡ºè¨Šæ¯ä¸¦ç²å–å›æ‡‰
        :param message: ä½¿ç”¨è€…è¼¸å…¥çš„è¨Šæ¯
        :return: AI å›æ‡‰
        """
        self.messages_history.append({"role": "user", "content": message})

        # æº–å‚™å°è©±å…§å®¹
        messages = self.messages_history

        # é¸æ“‡å°æ‡‰çš„ AI æ¨¡å‹
        if self.ai_model.startswith("gpt"):
            response = await self.clientGPT.chat.completions.create(
                model=self.ai_model,
                messages=messages,
                temperature=0.7
            )
            reply = response.choices[0].message.content

        elif self.ai_model.startswith("claude"):
            response = await self.clientCLAUDE.messages.create(
                model=self.ai_model,
                max_tokens=8192,
                temperature=0.7,
                messages=messages
            )
            reply = response.content[0].text

        else:
            reply = "âš ï¸ ä¸æ”¯æ´çš„ AI æ¨¡å‹"

        # è¨˜éŒ„ AI å›æ‡‰
        self.messages_history.append({"role": "assistant", "content": reply})
        return reply
    async def assistant_normal(self, query="", role_context="", task_context="", tone_context="", input_data="", examples="",
                         task_description="", immediate_task="", precognition="", output_formatting="", answer_tag="",
                         return_all_message=False, add_response_to_ui=True):
        ######################################## æç¤ºå…ƒç´  ########################################
        def prompt_creation():
            ##### æç¤ºå…ƒç´  1ï¼š`user` è§’è‰²
            ROLE_CONTEXT = f"""{role_context}"""

            ##### æç¤ºå…ƒç´  2ï¼šä»»åŠ¡ä¸Šä¸‹æ–‡
            TASK_CONTEXT = f"""{task_context}"""

            ##### æç¤ºå…ƒç´  3ï¼šè¯­æ°”ä¸Šä¸‹æ–‡
            TONE_CONTEXT = f"""{tone_context}"""

            ##### æç¤ºå…ƒç´  4ï¼šè¦å¤„ç†çš„è¾“å…¥æ•°æ®
            INPUT_DATA = f"""<input_data>{input_data}</input_data>"""

            ##### æç¤ºå…ƒç´  5ï¼šç¤ºä¾‹
            EXAMPLES = f"""{examples}"""

            ##### æç¤ºå…ƒç´  6ï¼šè¯¦ç»†çš„ä»»åŠ¡æè¿°å’Œè§„åˆ™
            TASK_DESCRIPTION = f"""<query>{query}</query>{task_description}â€"""

            ##### æç¤ºå…ƒç´  7ï¼šç«‹å³ä»»åŠ¡æè¿°æˆ–è¯·æ±‚
            IMMEDIATE_TASK = f"""{immediate_task}"""

            ##### æç¤ºå…ƒç´  8ï¼šé¢„æƒ³ï¼ˆé€æ­¥æ€è€ƒï¼‰
            PRECOGNITION = f"""{precognition}"""

            ##### æç¤ºå…ƒç´  9ï¼šè¾“å‡ºæ ¼å¼
            OUTPUT_FORMATTING = f"""
            <{answer_tag}>
            {output_formatting}
            </{answer_tag}>
            """

            ##### æç¤ºå…ƒç´  10ï¼šé¢„å¡«å…… Claude çš„å›å¤ï¼ˆå¦‚æœæœ‰ï¼‰
            PREFILL = f"<{answer_tag}>"

            PROMPT = prompt_combined(ROLE_CONTEXT, TASK_CONTEXT, TONE_CONTEXT, INPUT_DATA, EXAMPLES, TASK_DESCRIPTION,
                                     IMMEDIATE_TASK, PRECOGNITION, OUTPUT_FORMATTING)

            return PROMPT, PREFILL

        prompt, prefill = prompt_creation()
        response_message = await self.ai_model_answer_management(query, prompt, prefill, return_all_message,
                                                      self.ai_model, answer_tag=answer_tag, add_ids=True,
                                                      add_response_to_ui=add_response_to_ui)
        return f"{prefill}{response_message}"
    async def assistant_with_search_tool(self, query="", role_context="", task_context="", tone_context="", input_data="",
                                   examples="", task_description="", immediate_task="", precognition="",
                                   output_formatting="", pinecone_index="", answer_tag="", threshold=0.5,
                                   top_k=100, return_all_message=False, filename=None, add_response_to_ui=True, callback=None):
        ######################################## æç¤ºå…ƒç´  ########################################
        def prompt_creation(search_data):
            ##### æç¤ºå…ƒç´  1ï¼š`user` è§’è‰²
            ROLE_CONTEXT = f"""{role_context}"""

            ##### æç¤ºå…ƒç´  2ï¼šä»»åŠ¡ä¸Šä¸‹æ–‡
            TASK_CONTEXT = f"""{task_context}"""

            ##### æç¤ºå…ƒç´  3ï¼šè¯­æ°”ä¸Šä¸‹æ–‡
            TONE_CONTEXT = f"""{tone_context}"""

            ##### æç¤ºå…ƒç´  4ï¼šè¦å¤„ç†çš„è¾“å…¥æ•°æ®
            INPUT_DATA = f"""
            <input_data>
            {input_data}
            <search_data>
            {search_data}
            </search_data>
            </input_data>
            """

            ##### æç¤ºå…ƒç´  5ï¼šç¤ºä¾‹
            EXAMPLES = f"""{examples}"""

            ##### æç¤ºå…ƒç´  6ï¼šè¯¦ç»†çš„ä»»åŠ¡æè¿°å’Œè§„åˆ™
            TASK_DESCRIPTION = f"""<query>{query}</query>{task_description}â€"""

            ##### æç¤ºå…ƒç´  7ï¼šç«‹å³ä»»åŠ¡æè¿°æˆ–è¯·æ±‚
            IMMEDIATE_TASK = f"""{immediate_task}"""

            ##### æç¤ºå…ƒç´  8ï¼šé¢„æƒ³ï¼ˆé€æ­¥æ€è€ƒï¼‰
            PRECOGNITION = f"""{precognition}"""

            ##### æç¤ºå…ƒç´  9ï¼šè¾“å‡ºæ ¼å¼
            OUTPUT_FORMATTING = f"""
            <{answer_tag}>
            {output_formatting}
            </{answer_tag}>
            """

            ##### æç¤ºå…ƒç´  10ï¼šé¢„å¡«å…… Claude çš„å›å¤ï¼ˆå¦‚æœæœ‰ï¼‰
            PREFILL = "" if answer_tag == "" else f"<{answer_tag}>"


            PROMPT = prompt_combined(ROLE_CONTEXT, TASK_CONTEXT, TONE_CONTEXT, INPUT_DATA, EXAMPLES, TASK_DESCRIPTION,
                                     IMMEDIATE_TASK, PRECOGNITION, OUTPUT_FORMATTING)

            return PROMPT, PREFILL

        ######################################## è³‡æ–™æœç´¢ ########################################

        if pinecone_index != "":
            response_message = self.assistant_normal(
                query=f"åˆ†æä½¿ç”¨è€…çš„å•é¡Œï¼šã€Œ{query}ã€ï¼Œå¾ <input_data> ä¸­æå–èˆ‡å•é¡Œç›¸é—œçš„å…§å®¹ï¼Œä¸¦æ”¾ç½®åœ¨ <reference> æ¨™ç±¤ä¸­ã€‚å¦‚æœè³‡æ–™ä¸è¶³ä»¥å›ç­”å•é¡Œï¼Œå‰‡é€²ä¸€æ­¥åˆ¤æ–·æ˜¯å¦éœ€è¦åˆ° Pinecone è³‡æ–™åº«æª¢ç´¢ï¼Œä¸¦å°‡ç²¾æº–çš„æœå°‹èªå¥æ”¾ç½®æ–¼ <search_word> ä¸­ï¼›è‹¥åˆ¤å®šç„¡éœ€æª¢ç´¢ï¼Œå‰‡æ¨™è¨˜ã€Œç•¥éæœç´¢ã€ã€‚",
                role_context="ä½ æ˜¯ä¸€ä½å°ˆç²¾æ–¼è³‡æ–™æª¢ç´¢èˆ‡èªæ„åˆ†æçš„åŠ©æ‰‹ï¼Œèƒ½å¤ æ ¹æ“šç”¨æˆ¶å•é¡Œçš„æ€§è³ªéˆæ´»æ±ºå®šæ˜¯å¦éœ€è¦é€²è¡Œè³‡æ–™æœç´¢ã€‚",
                task_context=f"""
                                    ç”¨æˆ¶å‘ FreeAger AI æå•ï¼ŒFreeAger æ˜¯å¹«åŠ©é•·è€…çªç ´å¹´é½¡é™åˆ¶ä¸¦æ¢ç´¢äººç”Ÿæ„ç¾©çš„åŠ©æ‰‹ã€‚
                                    ä½ çš„ä¸»è¦ä»»å‹™æ˜¯å¹«åŠ© FreeAger AI æ•´ç†ç›¸é—œè³‡æ–™å’Œåˆ¤æ–·è‹¥éœ€é€²éšæœç´¢çš„ç²¾ç¢ºè³‡æ–™æª¢ç´¢è©èªï¼š
                                    1. åˆ†æç”¨æˆ¶å•é¡Œï¼Œåˆ¤æ–·å…¶æ ¸å¿ƒéœ€æ±‚æ˜¯å¦èˆ‡çŸ¥è­˜æª¢ç´¢ç›¸é—œã€‚
                                    2. åœ¨ <input_data> ä¸­æª¢ç´¢èˆ‡å•é¡Œç›¸é—œçš„è³‡è¨Šã€‚
                                    3. åˆ¤æ–·æª¢ç´¢è³‡æ–™æ˜¯å¦è¶³ä»¥å›ç­”å•é¡Œï¼š
                                       - è‹¥è¶³å¤ ï¼Œç›´æ¥ç”Ÿæˆå›ç­”ï¼Œä¸¦æ¨™è¨˜ <search_word> ç‚ºã€Œç•¥éæœç´¢ã€ã€‚
                                       - è‹¥ä¸è¶³ï¼Œé€²ä¸€æ­¥åˆ¤æ–·æ˜¯å¦éœ€è¦åˆ° Pinecone è³‡æ–™åº«æª¢ç´¢ï¼š
                                         a. è‹¥éœ€è¦ï¼Œç”Ÿæˆé©åˆèªæ„å‘é‡æœç´¢çš„ç²¾æº–èªå¥ï¼Œæ”¾ç½®æ–¼ <search_word>ã€‚
                                         b. è‹¥ä¸éœ€è¦ï¼ˆå¦‚å•é¡Œä¸æ¶‰åŠæª¢ç´¢æˆ–åªæ˜¯ç°¡å–®äº’å‹•ï¼‰ï¼Œæ¨™è¨˜ <search_word> ç‚ºã€Œç•¥éæœç´¢ã€ã€‚
                                """,
                tone_context="è«‹ä¿æŒå°ˆæ¥­ä¸”ç°¡æ½”çš„èªæ°£ï¼Œæ ¹æ“šç”¨æˆ¶å•é¡Œéˆæ´»èª¿æ•´å›ç­”ã€‚",
                input_data=f"",
                examples="",
                task_description=f"""
                                    å›ç­”ç”¨æˆ¶å•é¡Œæ™‚è«‹æŒ‰ç…§ä»¥ä¸‹æ­¥é©Ÿï¼š
                                    1. åˆ†æå•é¡Œæ„åœ–ï¼Œæ€è€ƒè¦å›ç­”é€™å€‹å•é¡Œéœ€è¦é‚£äº›è³‡æ–™åƒè€ƒï¼Œå°‡éœ€è¦çš„è³‡æ–™æ–¹å‘åˆ—é»æ–¼ <required_information> æ¨™ç±¤å…§ã€‚
                                    2. ä¾ç…§å•é¡Œéœ€æ±‚åœ¨ <input_data> ä¸­æª¢ç´¢ç›¸é—œè³‡è¨Šï¼Œç›¡å¯èƒ½å°‡ç›¸é—œçš„çµæœéƒ½æ”¾ç½®æ–¼ <reference> æ¨™ç±¤å…§ã€‚
                                    3. åˆ¤æ–· <reference> ä¸­çš„è³‡è¨Šæ˜¯å¦è¶³å¤ å®Œæ•´å›ç­”å•é¡Œï¼š
                                       - è‹¥è¶³å¤ ï¼Œç›´æ¥ç”Ÿæˆç­”æ¡ˆï¼Œä¸¦æ–¼ <search_word> ä¸­æ¨™è¨˜ã€Œç•¥éæœç´¢ã€ã€‚
                                       - è‹¥ä¸è¶³ï¼Œæ ¹æ“šå•é¡Œæ€§è³ªé€²è¡Œåˆ¤æ–·ï¼š
                                         a. å¦‚æœéœ€è¦æª¢ç´¢æ›´å¤šè³‡æ–™ï¼Œç”Ÿæˆé©åˆ Pinecone çš„æœå°‹èªå¥ä¸¦æ”¾ç½®æ–¼ <search_word>ã€‚Pineconeè³‡æ–™åº«ä½¿ç”¨èªç¾©æœç´¢ï¼Œå› æ­¤æœå°‹èªå¥ä½¿ç”¨å®Œæ•´çš„å¥å­ï¼Œè€Œä¸æ˜¯é—œéµå­—é¡å‹ã€‚æœå°‹èªå¥ä¸­çš„å­¸è¡“é—œéµå­—è«‹ç”¨ä¸­æ–‡æ­é…(è‹±æ–‡)ä¾†å‘ˆç¾ï¼Œä¾‹å¦‚ï¼šç©å¿ƒ(playfullness)ã€‚æœå°‹ç‰¹å®šé—œéµå­—å­¸è¡“å®šç¾©æ™‚ï¼Œèªå¥ä¸­ä¸ç”¨ç‰¹æ„åŠ ä¸Šfreeageræ¨¡å‹åœ–ï¼Œæœƒå°è‡´æœå°‹å‘é‡å¤±ç„¦ã€‚
                                         b. å¦‚æœå•é¡Œæœ¬è³ªä¸éœ€è¦æª¢ç´¢ï¼Œä¾‹å¦‚ç”¨æˆ¶å•å€™æˆ–äº’å‹•å•é¡Œï¼Œæ¨™è¨˜ <search_word> ç‚ºã€Œç•¥éæœç´¢ã€ã€‚
                                    4. ä½ åªéœ€å›ç­” <reference> å’Œ <search_word> æ¨™ç±¤ä¸­çš„å…§å®¹ï¼Œä¸éœ€è¦å›ç­”å…¶ä»–çš„è§£é‡‹å’Œèªªæ˜ã€‚     
                                """,
                immediate_task="åˆ†æå•é¡Œä¸¦æª¢ç´¢ç›¸é—œè³‡æ–™ï¼Œæ ¹æ“šéœ€è¦ç”Ÿæˆæœå°‹èªå¥æˆ–æ¨™è¨˜ç•¥éæœç´¢ã€‚",
                precognition="",
                output_formatting=f"""
                                    <reference>...</reference>
                                    <search_word>...</search_word>
                                """,
                answer_tag=f"preprocess_{query}",
                return_all_message=True,
                add_response_to_ui=False
            )
        else:
            response_message = "<reference>ç•¥éæœç´¢</reference>"

        preprocess_reference = extract_tagged_content_from_str(response_message, "reference")[0]
        if "ç•¥éæœç´¢" in response_message:
            print("æ­¤é¡Œä¸éœ€è¦æœç´¢ï¼Œç•¥épineconeæœç´¢")
            search_box = ["ç„¡æœå°‹åƒè€ƒè³‡æ–™ï¼Œè«‹ç•¥é"]
        else:
            picone_search_topic = extract_tagged_content_from_str(response_message, "search_word")[0]
            print(f"picone_search_topic:{picone_search_topic}")
            search_box = self.search_tool(f"{picone_search_topic}", picone_index=pinecone_index, token_budget=7000,
                                     top_k=top_k, filename=filename, threshold=threshold)
        # å¦‚æœ search_tool è¿”å›ç©ºåˆ—è¡¨ï¼Œæä¾›é»˜èªå€¼
        if not search_box:
            search_box = ["æ²’æœ‰è¶…éé–¥å€¼ä»¥ä¸Šçš„æœå°‹çµæœ"]

        print(f"=== search_box å…§æœ‰ {len(search_box)} ç­†è³‡æ–™ ===")
        progress = 0
        final_data = ""
        for search_data in search_box:
            if search_data in ["ç„¡æœå°‹åƒè€ƒè³‡æ–™ï¼Œè«‹ç•¥é", "æ²’æœ‰è¶…éé–¥å€¼ä»¥ä¸Šçš„æœå°‹çµæœ"]:
                break
            progress = progress + 1
            print(f"=== ç›®å‰æ­£åœ¨é€²è¡Œ search_box ä¸­çš„ç¬¬  {progress}/{len(search_box)} ç­†è³‡æ–™ ===")
            self.assistant_normal(query=f"æå–å‡ºèˆ‡ã€Œ{query}ã€ç›¸é—œçš„å¼•æ–‡",
                             role_context="ä½ æ˜¯ä¸€ä½èƒ½å¾å¤§é‡è³‡æ–™ä¸­æ•´ç†å’Œåˆ†æèˆ‡ä¸»é¡Œç›¸é—œè³‡æ–™çš„AIæ©Ÿå™¨äººï¼Œå°ˆæ³¨æ–¼æä¾›ç›¸é—œçš„åˆ†æèˆ‡è§£é‡‹ã€‚",
                             task_context=f"ä¾æ“š <input_data> æ¨™ç±¤ä¸­çš„è¼¸å…¥è³‡æ–™ç‚ºä¾†æºï¼Œå°‡èˆ‡ ã€Œ{query}ã€ ç›¸é—œçš„å¼•æ–‡å…§å®¹é€æ­¥åˆ—å‡ºã€‚",
                             tone_context="",
                             input_data=f"{search_data}",
                             examples=f"""
                                å°‡ä½ æ–¼ <input_data> ä¸­åˆ†æå‡ºæ‰€æœ‰ç›¸é—œå¼•è¨€æ”¾åœ¨ <search_result_file> æ¨™ç±¤ä¸­ã€‚å¦å¤–ï¼Œå°‡åŸå§‹å¼•æ–‡æ”¾åœ¨ <original> æ¨™ç±¤ï¼Œä¸¦å°‡å…¶ç¹é«”ä¸­æ–‡çš„èªªæ˜æ”¾åœ¨ <explain> æ¨™ç±¤ã€‚

                                <example>
                                 <search_result_file = [33-178] goodstein1978.pdf>
                                 <original>
                                 The definition of self-esteem is a person's overall evaluation and perception of their own worth.(Brown, 1993, 1998; Brown & Dutton, 1995)
                                 </original>
                                 <explain>
                                 è‡ªå°Šçš„å®šç¾©æ˜¯ä¸€å€‹äººå°è‡ªå·±åƒ¹å€¼çš„æ•´é«”è©•åƒ¹èˆ‡çœ‹æ³•ã€‚ (Brown, 1993, 1998; Brown & Dutton, 1995)
                                 </explain>
                                 </search_result_file>
                                </example>
                                """,
                             task_description=f"""å›ç­”å•é¡Œæ™‚è«‹ä¾ç…§ä¸‹åˆ—æ­¥é©Ÿé€²è¡Œï¼š
                                1. æ ¹æ“š ã€Œ{query}ã€ çš„ä¸»é¡Œï¼Œå° <input_data> ä¸­çš„è³‡æ–™é€²è¡Œåˆ†æï¼Œæ‰¾å‡ºç›¸é—œçš„å¼•è¨€å…§å®¹ã€‚
                                2. å°‡åŸå§‹å¼•è¨€æ”¾ç½®åœ¨ <original> æ¨™ç±¤ä¸­ã€‚
                                3. å°‡åŸå§‹å¼•è¨€çš„ç¹é«”ä¸­æ–‡è§£é‡‹ç‰ˆæœ¬æ”¾ç½®åœ¨ <explain> æ¨™ç±¤ä¸­ã€‚
                                4. å°‡ <original> å’Œ <explain> çš„æˆæœï¼Œæ”¾ç½®åœ¨ <search_result_file> æ¨™ç±¤ä¸­ã€‚
                                5. åƒ…é™æ–¼æå–é‡è¦å¼•æ–‡è§£é‡‹ï¼Œé¿å…æ·»åŠ ç„¡é—œçš„å…§å®¹ã€‚""",
                             immediate_task="",
                             precognition="",
                             output_formatting=f"""
                                <search_result_file = [33-178] goodstein1978.pdf>
                                <original>
                                The definition of self-esteem is a person's overall evaluation and perception of their own worth.(Brown, 1993, 1998; Brown & Dutton, 1995)
                                </original>
                                <explain>
                                è‡ªå°Šçš„å®šç¾©æ˜¯ä¸€å€‹äººå°è‡ªå·±åƒ¹å€¼çš„æ•´é«”è©•åƒ¹èˆ‡çœ‹æ³•ã€‚ (Brown, 1993, 1998; Brown & Dutton, 1995)
                                </explain>
                                </search_result_file>

                                <search_result_file = [70-5] Article Self-Concept by Saul McLeod.pdf-2>
                                <original>
                                Self-esteem refers to the extent to which we like, accept, or approve of ourselves, or how much wevalue ourselves. Self-esteem always involves a degree of evaluation and we may have either a positive
                                or a negative view of ourselves.
                                </original>
                                <explain>
                                è‡ªå°Šæ˜¯æŒ‡æˆ‘å€‘å–œæ­¡ã€æ¥å—æˆ–èªå¯è‡ªå·±çš„ç¨‹åº¦ï¼Œæˆ–è€…æ˜¯æˆ‘å€‘å°è‡ªå·±åƒ¹å€¼çš„è©•ä¼°ã€‚è‡ªå°Šç¸½æ˜¯æ¶‰åŠæŸç¨®ç¨‹åº¦çš„è©•åƒ¹ï¼Œæˆ‘å€‘å°è‡ªå·±å¯èƒ½æŒæœ‰æ­£é¢çš„æˆ–è² é¢çš„çœ‹æ³•ã€‚
                                </explain>
                                </search_result_file>""",
                             answer_tag=f"data_{query}",
                             return_all_message=True,
                             add_response_to_ui=False)
            final_data = extract_tagged_content_from_messages(self.messages_history_ai, f"data_{query}")
        ######################################## AIæå• ########################################
        prompt, prefill = prompt_creation(f"{preprocess_reference}\n{final_data}")
        response_message = await self.ai_model_answer_management(query, prompt, prefill, return_all_message,
                                                      self.ai_model,
                                                      answer_tag=answer_tag, add_ids=True,
                                                      add_response_to_ui=add_response_to_ui)
        if callback:
            callback(prefill, response_message)
        return f"{prefill}{response_message}"
    async def ai_model_answer_management(self, message_for_ui, message_for_ai, prefill, return_all_message, ai_model,
                                   temperature=0, answer_tag="", add_ids=False, add_response_to_ui=True):
        if self.print_prompt == True:
            print(f"""
            =======================================================================================================================
            æ­¤æ¬¡çš„PROMPT:
            {message_for_ai}
            =======================================================================================================================
            """)
        # åˆ¤æ–·é€™æ¬¡è¦ä¸Ÿçµ¦AIè™•ç†çš„å°è©±
        messages_for_AImodel = []
        # åˆ¤æ–·æ˜¯å¦è¦åŠ ä¸Šè¨“ç·´ç”¨å°è©±
        if self.training_message:
            messages_for_AImodel.extend(self.training_message)
        # åˆ¤æ–·æ˜¯å¦è¦åŠ ä¸Šå…¨éƒ¨è¨Šæ¯å…¨éƒ¨è¨Šæ¯
        if return_all_message == True:
            messages_for_AImodel.extend(self.messages_history_ai)
        # åŠ ä¸Šæ­¤æ¬¡è¦çµ¦çš„è¨Šæ¯
        messages_for_AImodel.append({"role": "user", "content": message_for_ai})
        if prefill != "":
            messages_for_AImodel.append({"role": "assistant", "content": prefill})

        # å›ç­”å•é¡Œ
        if "claude" in ai_model.lower():
            response = await self.clientCLAUDE.messages.create(
                model=ai_model,
                max_tokens=8192,
                temperature=temperature,
                system="",
                messages=messages_for_AImodel
            )
            response_message = response.content[0].text
        elif ai_model == 'grok-vision-beta':

            response = self.clientXAI.messages.create(
                model=ai_model,
                max_tokens=8192,
                temperature=temperature,
                messages=messages_for_AImodel
            )
            response_message = response.content[0].text
        else:
            if ai_model == 'o1-preview':
                temperature = 1
            response = await self.clientGPT.chat.completions.create(
                model=ai_model,
                messages=messages_for_AImodel,
                temperature=temperature
            )
            response_message = response.choices[0].message.content

        print('ğŸ¤– ========= AI model å›è¦†è¨Šæ¯ =====================')
        if add_ids == True:
            response_message = add_reference_ids(response_message)
        print(f"{prefill}\n{response_message}")
        
        # ========ç®¡ç†å°è©±ç´€éŒ„=============
        if answer_tag != "" and extract_tagged_content_from_str(response_message, answer_tag) == "":
            assistant_content = f"<{answer_tag}>\n{prefill}{response_message}\n</{answer_tag}>"
        else:
            assistant_content = f"{prefill}{response_message}"
        # ç®¡ç†çµ¦aiçš„å°è©±ç´€éŒ„
        self.messages_history_ai.append({"role": "user", "content": message_for_ui})
        self.messages_history_ai.append({"role": "assistant", "content": assistant_content})
        # ç®¡ç†çµ¦uiçš„å°è©±ç´€éŒ„
        if add_response_to_ui == True:
            self.messages_history_ui.append({"role": "user", "content": message_for_ui})
            self.messages_history_ui.append({"role": "assistant", "content": assistant_content})
       
        return response_message
    def search_tool(self, query: str, token_budget: int, top_k=10, picone_index="freeager-scholar-middle-chunk",
                    filename=None, threshold=0.5) -> list:
        """Return a list of message strings, with relevant source texts pulled from a dataframe."""

        # Query articles using the provided query
        strings, relatednesses, indices, filenames, sections = self.pinecone_query_article(query, picone_index=picone_index,
                                                                                      filename=filename, top_k=top_k,
                                                                                      threshold=threshold)

        # æª¢æŸ¥æ˜¯å¦æœ‰ä»»ä½•ç›¸é—œæ€§é”åˆ°æ¨™æº–çš„çµæœ
        if not strings or strings == ['N/A']:
            print('No matches above the threshold.')
            return []

        # Create an empty list to store messages
        messages = []
        current_message = ""

        # å‰µå»ºç©ºçš„ DataFrame
        df = pd.DataFrame(columns=['Relatedness', 'FileName-section', 'Text'])

        # Process each result and add to the message string
        for string, relatedness, index, filename, section in zip(strings, relatednesses, indices, filenames, sections):
            next_article = f'\n<search_result_file = {filename}-{section}>\n{string}\n</search_result>\n'

            # If adding the next article exceeds the token limit, finalize the current message and start a new one
            if num_tokens(current_message + next_article, model="gpt-3.5-turbo") > token_budget:
                # Append the current message to the messages list
                messages.append(current_message)
                # Reset current_message to start a new one
                current_message = next_article
            else:
                # If it doesn't exceed the token budget, keep adding to the current message
                current_message += next_article

            # Add data to the DataFrame for reference table
            new_row = {
                'Relatedness': relatedness,
                'FileName-section': f"{filename}-{section}",
                'Text': string.replace('\n', ' ')
            }
            df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)

        # Append the final message if there's remaining content
        if current_message:
            messages.append(current_message)

        # é¡¯ç¤ºçµæœçš„DataFrameè¡¨æ ¼
        print(f"search_result_df:{df}")
        # Return the list of message strings
        return messages
    async def pinecone_query_article(self, query, picone_index="freeager-scholar-middle-chunk", filename=None, top_k=5,
                               threshold=0.6):

        '''Queries an article using the provided query and prints results.'''
        print('=== è‡³ pinecone æœå°‹è³‡æ–™ä¸­ ===')

        # Create vector embeddings based on the query
        query_embedding_response = await self.clientGPT.embeddings.create(
            model="text-embedding-3-small",
            input=query,
        )
        embedded_query = query_embedding_response.data[0].embedding
        # æª¢æŸ¥æ˜¯å¦æœ‰æŒ‡å®š filename ä¸¦è¨­ç½®ç¯©é¸æ¢ä»¶
        filter_condition = {}
        if filename:
            filter_condition = {'filename': {'$eq': filename}}

        index_name = picone_index
        print(f"index_name:{index_name}")

        index = self.pc.Index(name=index_name)
        # Query the index using the query vector and apply filter by filename
        query_result = index.query(
            vector=embedded_query,
            namespace='content',
            top_k=top_k,
            filter=filter_condition  # ä½¿ç”¨ filter åƒæ•¸ä¾†æ ¹æ“š filename ç¯©é¸
        )

        # Print query results
        if not query_result.matches:
            print('No query result')
            return

        matches = query_result.matches
        indices = [str(res.id) for res in matches]
        relatednesses = [res.score for res in matches]
        # Fetch metadata for the matched vectors
        response = index.fetch(ids=indices, namespace='content')

        strings, sections, filenames = [], [], []
        for vector_id in indices:
            if 'vectors' in response and vector_id in response['vectors']:
                vector_info = response['vectors'][vector_id]
                if 'metadata' in vector_info:
                    metadata = vector_info['metadata']
                    strings.append(metadata.get('text', 'N/A'))
                    sections.append(metadata.get('section', 'N/A'))
                    filenames.append(metadata.get('filename', 'N/A'))
                else:
                    print(f"No metadata found for vector ID {vector_id}")
                    strings.append('N/A')
                    sections.append('N/A')
                    filenames.append('N/A')
            else:
                print(f"No vector found for ID {vector_id} in namespace {'content'}")
                strings.append('N/A')
                sections.append('N/A')
                filenames.append('N/A')

        df = pd.DataFrame({
            'indices': indices,
            'relatednesses': relatednesses,
            'strings': strings,
            'filenames': filenames,
            'sections': sections
        })

        # è¿‡æ»¤æ‰ç›¸å…³æ€§å°äºé˜ˆå€¼çš„è¡Œ
        df = df[df['relatednesses'] >= threshold]

        # æ‰“å°è¿‡æ»¤åçš„DataFrame
        # print(df.head(30))

        # å¦‚æœæ²¡æœ‰ä»»ä½•ç»“æœè¶…å‡ºé˜ˆå€¼ï¼Œè¿”å›ç©ºåˆ—è¡¨
        if df.empty:
            print('No matches above the threshold.')
            return [], [], [], [], []
        # Return all results in the desired format
        strings = df['strings'].tolist()
        relatednesses = df['relatednesses'].tolist()
        indices = df['indices'].tolist()
        filenames = df['filenames'].tolist()
        sections = df['sections'].tolist()

        return strings, relatednesses, indices, filenames, sections
    async def save_messages_to_postgresql(self) -> None:
        """å°‡ messages_history_ui & messages_history_ai æ‰¹æ¬¡å¯«å…¥ Conversation è³‡æ–™è¡¨"""
        user_id = int(self.bot_name)
        # å°‡å…©æ¢æ­·å²åˆä½µï¼›è‹¥ä½ åªæƒ³å­˜å…¶ä¸­ä¸€æ¢ï¼Œè‡ªè¡Œæ›¿æ›
        full_history = self.messages_history_ui + self.messages_history_ai
        if not full_history:
            print("ğŸ’¡ ç„¡è¨Šæ¯å¯å¯«å…¥ PostgreSQLï¼Œç•¥éã€‚")
            return

        # DataFrame æ–¹ä¾¿è½‰æ› / æ¿¾é™¤ç„¡æ•ˆæ¬„ä½
        df = pd.DataFrame(full_history)
        records = [
            {
                "user_id": user_id,
                "role": row.get("role", "assistant"),
                "message": row.get("content", ""),
                "timestamp": datetime.utcnow(),
            }
            for _, row in df.iterrows()
        ]

        async with AsyncSessionLocal() as session:
            async with session.begin():
                await session.execute(insert(Conversation), records)
            # session.begin() æœƒè‡ªå‹• commit
        print(f"âœ… å·²å¯«å…¥ {len(records)} ç­†å°è©±åˆ° PostgreSQL")
    def clear_all_messages(self):
        self.messages_history_ai = []
        self.messages_history_ui = []


def prompt_combined(ROLE_CONTEXT, TASK_CONTEXT, TONE_CONTEXT, INPUT_DATA, EXAMPLES, TASK_DESCRIPTION, IMMEDIATE_TASK,
                    PRECOGNITION, OUTPUT_FORMATTING):
    PROMPT = ""

    if ROLE_CONTEXT:
        PROMPT += f"""{ROLE_CONTEXT}"""

    if TASK_CONTEXT:
        PROMPT += f"""\n\n{TASK_CONTEXT}"""

    if TONE_CONTEXT:
        PROMPT += f"""\n\n{TONE_CONTEXT}"""

    if INPUT_DATA:
        PROMPT += f"""\n\n{INPUT_DATA}"""

    if EXAMPLES:
        PROMPT += f"""\n\n{EXAMPLES}"""

    if TASK_DESCRIPTION:
        PROMPT += f"""\n\n{TASK_DESCRIPTION}"""

    if IMMEDIATE_TASK:
        PROMPT += f"""\n\n{IMMEDIATE_TASK}"""

    if PRECOGNITION:
        PROMPT += f"""\n\n{PRECOGNITION}"""

    if OUTPUT_FORMATTING:
        PROMPT += f"""\n\n{OUTPUT_FORMATTING}"""


    return PROMPT
def get_filepath_to_messages(message_file_path):
    if os.path.exists(message_file_path):
        # è®€å– CSV æª”æ¡ˆä¸¦è½‰æ›æˆå°æ‡‰çš„æ ¼å¼
        messages_history_df = pd.read_csv(message_file_path)
        messages = messages_history_df.to_dict(orient='records')
    # æª¢æŸ¥æœ€å¾Œä¸€ç­†è³‡æ–™çš„ roleï¼Œå¦‚æœæ˜¯ 'user'ï¼Œå‰‡åˆªé™¤
    if messages and messages[-1]["role"] == "user":
        messages.pop()
    return messages
def extract_tagged_content_from_str(input_str, tag_name):
    """
    æå–æŒ‡å®šæ¨™ç±¤ä¸­çš„å…§å®¹ï¼Œä¸¦æ¸…ç†å…§å®¹ä¸­çš„å¤šé¤˜ç©ºç™½ã€‚

    åƒæ•¸:
    input_str (str): åŒ…å«ä¸€å€‹æˆ–å¤šå€‹ç‰¹å®šæ¨™ç±¤çš„å­—ç¬¦ä¸²ã€‚
    tag_name (str): éœ€è¦æå–å…§å®¹çš„æ¨™ç±¤åç¨±ã€‚

    è¿”å›:
    List[str]: åŒ…å«æ‰€æœ‰æå–å‡ºä¾†ä¸¦æ¸…ç†éçš„æ¨™ç±¤å…§å®¹çš„åˆ—è¡¨ã€‚
    """
    pattern = rf'<{tag_name}\s*[^>]*>(.*?)</{tag_name}>'
    # ä½¿ç”¨ re.DOTALL ä¾†è·¨è¡ŒåŒ¹é…
    contents = re.findall(pattern, input_str, re.DOTALL)
    # æ¸…ç†æå–å‡ºçš„å…§å®¹
    cleaned_contents = [' '.join(content.split()) for content in contents]
    return cleaned_contents
def extract_tagged_content_from_messages(messages, tag, only_extract_first=False):
    """
    æå–å°è©±ç´€éŒ„ä¸­ï¼ŒæŒ‡å®šæ¨™ç±¤(tag)çš„æ‰€æœ‰å…§å®¹ã€‚

    :param messages: åŒ…å«å¤šå€‹è¨Šæ¯çš„åˆ—è¡¨ï¼Œæ¯å€‹è¨Šæ¯æ˜¯å­—å…¸å½¢å¼
    :param tag: è¦æå–å…§å®¹çš„æ¨™ç±¤åç¨±
    :param only_extract_first: æ˜¯å¦åƒ…æå–ç¬¬ä¸€å€‹æ¨™ç±¤çš„å…§å®¹ï¼Œé»˜èªç‚º False
    :return: åŒ…å«æ‰€æœ‰æå–å…§å®¹çš„å­—ä¸²
    """
    message_tagged_str = ""
    pattern = f"<{tag}>(.*?)</{tag}>"

    for message in messages:
        content = message.get('content', '')
        if isinstance(content, str):
            # ä½¿ç”¨æ­£å‰‡è¡¨é”å¼æ‰¾åˆ°æ‰€æœ‰ç¬¦åˆçš„æ¨™ç±¤å…§å®¹
            matches = re.findall(pattern, content, re.DOTALL)
            if matches:
                if only_extract_first:
                    # å¦‚æœåªæå–ç¬¬ä¸€å€‹ï¼ŒåŠ å…¥ç¬¬ä¸€å€‹åŒ¹é…ä¸¦åœæ­¢
                    message_tagged_str += matches[0].strip() + "\n\n"
                    return message_tagged_str  # æå–ç¬¬ä¸€å€‹å¾Œç«‹å³è¿”å›
                else:
                    # å¦å‰‡æå–æ‰€æœ‰åŒ¹é…å…§å®¹
                    for match in matches:
                        message_tagged_str += match.strip() + "\n\n"

    return message_tagged_str
def add_reference_ids(text):
    global global_reference_id  # ä½¿ç”¨å…¨åŸŸè®Šæ•¸

    # æ‰¾åˆ°æ‰€æœ‰ <search_result_file = XXXX>
    pattern = r"<search_result_file = (.+?)>"
    matches = re.findall(pattern, text)

    # æ›¿æ›æ¯å€‹åŒ¹é…çš„é …ç›®ï¼ŒåŠ ä¸Š reference_id ç·¨è™Ÿ
    for match in matches:
        new_tag = f"<search_result_file = reference_id_{global_reference_id}: {match}>"
        text = text.replace(f"<search_result_file = {match}>", new_tag, 1)
        global_reference_id += 1  # æ¯æ¬¡æ›¿æ›å¾Œéå¢å…¨åŸŸç·¨è™Ÿ

    return text
def num_tokens(text: str, model: str = "gpt-3.5-turbo") -> int:
    """Return the number of tokens in a string."""
    encoding = tiktoken.encoding_for_model(model)
    return len(encoding.encode(text))
def save_message_to_csv(messages_history, message_file_path):
    ensure_directory_exists(message_file_path)
    # å°‡ extended_messages_history è½‰æ›ç‚º DataFrame ä¸¦æ‰“å°
    messages_history_df = pd.DataFrame(messages_history)
    # å¦‚æœ DataFrame ä¸ç‚ºç©ºæ‰é€²è¡Œå­˜æª”
    if not messages_history_df.empty:
        messages_history_df.to_csv(message_file_path, index=False)
        print(f"å°è©±ç´€éŒ„å·²å­˜æª”è‡³ {message_file_path}")
    else:
        print("DataFrame æ˜¯ç©ºçš„ï¼Œæœªé€²è¡Œå­˜æª”ã€‚")
    print(f"""=== å°è©±æ­·å²ç´€éŒ„ === \n{messages_history_df}""")
def ensure_directory_exists(file_path):
    directory = os.path.dirname(file_path)
    if not os.path.exists(directory):
        os.makedirs(directory)
async def main():
        # å‰µå»ºä¸€å€‹ GPT-4o æ©Ÿå™¨äºº
        chatbot1 = AIChatbot(bot_name="BotA", ai_model="gpt-4o")
        chatbot1.read_previous_messages("./message_record/20241205.csv")
        # å‰µå»ºä¸€å€‹ Claude-3.5 æ©Ÿå™¨äºº
        chatbot2 = AIChatbot(bot_name="BotB", ai_model="claude-3-7-sonnet-20250219")

        print("BotA å°è©±ç´€éŒ„:", chatbot1.get_messages_history())
        print("BotB å°è©±ç´€éŒ„:", chatbot2.get_messages_history())
        response2 = """
        <prompt>
        æ ¹æ“šä»¥ä¸‹å°è©±å…§å®¹ï¼Œåˆ†æå…¶ä¸­éš±å«çš„æ ¸å¿ƒä¿¡å¿µã€æƒ…ç·’ç‹€æ…‹ï¼Œä»¥åŠå°è©±è€…çš„å¯èƒ½äººç”Ÿæ•…äº‹ã€‚è«‹ä¾ç…§ä»¥ä¸‹æ ¼å¼è¼¸å‡ºï¼š

        1. éš±å«ä¿¡å¿µç³»çµ±ï¼š
        - æ ¸å¿ƒåƒ¹å€¼è§€å’Œä¸–ç•Œè§€ï¼šå°è©±è€…å¦‚ä½•çœ‹å¾…ä¸–ç•Œä»¥åŠä»–å€‘èªç‚ºé‡è¦çš„åƒ¹å€¼
        - è‡ªæˆ‘èªçŸ¥å’Œèº«ä»½èªåŒï¼šå°è©±è€…å¦‚ä½•çœ‹å¾…è‡ªå·±ä»¥åŠä»–å€‘çš„è§’è‰²å’Œèº«ä»½
        - å°äººéš›é—œä¿‚çš„åŸºæœ¬å‡è¨­ï¼šå°è©±è€…èˆ‡ä»–äººäº’å‹•çš„åŸºæœ¬ä¿¡å¿µå’ŒæœŸå¾…
        
        2. æƒ…ç·’å…¨æ™¯ï¼š
        - ä¸»å°æƒ…ç·’åŠå…¶å¼·åº¦ï¼šç›®å‰ä¸»å°å°è©±è€…çš„æƒ…ç·’åŠå…¶å¼·çƒˆç¨‹åº¦
        - æƒ…ç·’è¡çªæˆ–çŸ›ç›¾ï¼šå°è©±è€…åœ¨æƒ…ç·’ä¸Šå¯èƒ½å­˜åœ¨çš„å…§åœ¨è¡çªæˆ–çŸ›ç›¾
        - æƒ…ç·’è®ŠåŒ–çš„é—œéµè§¸ç™¼é»å’Œæ‡‰å°æ¨¡å¼ï¼šå“ªäº›äº‹ä»¶æˆ–è©±èªå¼•ç™¼æƒ…ç·’è®ŠåŒ–ï¼Œå°è©±è€…å¦‚ä½•æ‡‰å°
        
        3. ç”Ÿå‘½æ•˜äº‹é‡æ§‹ï¼š
        - æ¨æ¸¬é—œéµç”Ÿæ´»ç¶“æ­·å’Œè½‰æŠ˜é»ï¼šå¯èƒ½å¡‘é€ å°è©±è€…ä¿¡å¿µå’Œè¡Œç‚ºçš„é‡å¤§äº‹ä»¶æˆ–æ™‚åˆ»
        - å½¢æˆçš„è¡Œç‚ºæ¨¡å¼å’Œæ€ç¶­ç¿’æ…£ï¼šç”±é€™äº›ç¶“æ­·æ‰€ç”¢ç”Ÿçš„è¡Œç‚ºå’Œæ€è€ƒæ–¹å¼
        - é€™äº›ç¶“æ­·å¦‚ä½•å½¢å¡‘ç•¶å‰çš„è¡Œç‚ºå’Œæ±ºç­–ï¼šéå»çš„ç¶“æ­·å¦‚ä½•å½±éŸ¿å°è©±è€…ç¾åœ¨çš„é¸æ“‡å’Œè¡Œå‹•
        
        4. åˆ†æè­‰æ“šï¼š
        - æä¾›å…·é«”çš„å°è©±å…§å®¹æˆ–èªå¥ä½œç‚ºæ”¯æŒåˆ†æçš„è­‰æ“šï¼Œä»¥å¢åŠ åˆ†æçš„å¯ä¿¡åº¦å’Œå¯¦ç”¨æ€§
        </prompt>
        ä½ æ˜¯ä¸€ä½promptå·¥ç¨‹å¸«ï¼Œæˆ‘æƒ³å¾å°è©±ä¸­ï¼Œåˆ†æå°è©±ä¸­éš±è—çš„ä¿¡å¿µã€æƒ…ç·’å’Œäººç”Ÿæ•…äº‹ã€‚æˆ‘çš„promptè©²æ€éº¼è¨­è¨ˆï¼Ÿè«‹å¹«æˆ‘å„ªåŒ–
        å°‡å„ªåŒ–éå¾Œçš„promptæ”¾ç½®æ–¼<better_prompt>ä¸­ã€‚
        """
        for i in range(5):
            print("=============================")
            print(f"ç¬¬ {i} è¼ªå°è©±")
            print("=============================")
            # ç™¼é€è¨Šæ¯
            response1 = await chatbot1.send_message(f"{response2}\næˆ‘æƒ³å¾å°è©±ä¸­ï¼Œåˆ†æå°è©±ä¸­éš±è—çš„ä¿¡å¿µã€æƒ…ç·’å’Œäººç”Ÿæ•…äº‹ã€‚æˆ‘çš„promptè©²æ€éº¼è¨­è¨ˆï¼Ÿè«‹å¹«æˆ‘å„ªåŒ–")
            print("BotA å›æ‡‰:\n", response1)
            response2 = await chatbot2.send_message(f"{response1}\næˆ‘æƒ³å¾å°è©±ä¸­ï¼Œåˆ†æå°è©±ä¸­éš±è—çš„ä¿¡å¿µã€æƒ…ç·’å’Œäººç”Ÿæ•…äº‹ã€‚æˆ‘çš„promptè©²æ€éº¼è¨­è¨ˆï¼Ÿè«‹å¹«æˆ‘å„ªåŒ–")
            print("BotB å›æ‡‰:\n", response2)


        # ç²å–å°è©±æ­·å²
        print("BotA å°è©±ç´€éŒ„:", chatbot1.get_messages_history())
        print("BotB å°è©±ç´€éŒ„:", chatbot2.get_messages_history())
if __name__ == '__main__':
    asyncio.run(main())
    

    """
    1. éš±å«ä¿¡å¿µç³»çµ±ï¼š
    - æ ¸å¿ƒåƒ¹å€¼è§€å’Œä¸–ç•Œè§€
    - è‡ªæˆ‘èªçŸ¥å’Œèº«ä»½èªåŒ
    - å°äººéš›é—œä¿‚çš„åŸºæœ¬å‡è¨­

    2. æƒ…ç·’å…¨æ™¯ï¼š
    - ä¸»å°æƒ…ç·’åŠå…¶å¼·åº¦
    - æƒ…ç·’è¡çªæˆ–çŸ›ç›¾
    - æƒ…ç·’è®ŠåŒ–çš„é—œéµè§¸ç™¼é»å’Œæ‡‰å°æ¨¡å¼

    3. ç”Ÿå‘½æ•˜äº‹é‡æ§‹ï¼š
    - æ¨æ¸¬é—œéµç”Ÿæ´»ç¶“æ­·å’Œè½‰æŠ˜é»
    - å½¢æˆçš„è¡Œç‚ºæ¨¡å¼å’Œæ€ç¶­ç¿’æ…£
    - é€™äº›ç¶“æ­·å¦‚ä½•å½¢å¡‘ç•¶å‰çš„è¡Œç‚ºå’Œæ±ºç­–



    """