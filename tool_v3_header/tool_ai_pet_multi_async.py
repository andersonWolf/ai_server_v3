# -*- coding: utf-8 -*-
"""
AIUserSession ï¼ PostgreSQL + pgvector ç‰ˆæœ¬
-------------------------------------------------
å·²å®Œå…¨ç§»é™¤ SQLiteï¼Œç›¸ä¾è³‡æ–™å…¨éƒ¨å¯«å…¥ `tool_postgreSQL.AnalysisSummary`ã€‚
FAISS ä»ç”¨æœ¬åœ°æª”æ¡ˆç´¢å¼•ï¼ˆè‹¥å¤šæ©Ÿéƒ¨ç½²è«‹æ”¹æ”¾ S3 æˆ– pgvector ANNï¼‰ã€‚
æ³¨æ„ï¼šåœ¨ Thread å…§éœ€ç”¨ `asyncio.run()` å»ºç«‹è‡¨æ™‚äº‹ä»¶è¿´åœˆä¾†åŸ·è¡Œ async å„²å­˜æµç¨‹ã€‚
"""
from __future__ import annotations
import asyncio
import os
import xml.etree.ElementTree as ET
from collections import defaultdict
from datetime import datetime
from typing import List
import pandas as pd
from dotenv import load_dotenv
from openai import AsyncOpenAI
from sqlalchemy import select, update
from sqlalchemy.ext.asyncio import AsyncSession
from contextlib import asynccontextmanager
from tool_async import tool_postgreSQL as db
from tool_async import tool_ai_chatbot_async as ai_chatbot
from tool_async import tool_ai_pet_prompt as prompt


load_dotenv()
# ------------------------------------
# AIUserSession
# ------------------------------------

class AIUserSession:
    """å°å–®ä¸€ user çš„å°è©±ã€åˆ†æèˆ‡è³‡æ–™å­˜å–ç®¡ç†"""
    def __init__(self, user_id: int, main_loop: asyncio.AbstractEventLoop | None = None, db_session: AsyncSession | None = None):
        self.main_loop = main_loop or asyncio.get_running_loop()
        self.user_id = user_id
        self.clientGPT = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))
        self.embedding_model = "text-embedding-3-small"
        self.message_analysis_threshold = 40
        self.temp_messages: List[dict] = []
        self.all_messages: List[dict] = []
        self.db_session = db_session  # FastAPI æœƒå‚³é€²ä¾†ï¼›èƒŒæ™¯è…³æœ¬ç‚º None

        # å°ˆå±¬èŠå¤©æ©Ÿå™¨äººï¼ˆçŸ­è¨Šæ¯åˆ†æï¼‰
        self.chatbot_short = ai_chatbot.AIChatbot(
            bot_name=str(user_id),
            ai_model="claude-3-7-sonnet-20250219",
        )
        self.chatbot_short.print_prompt = True
        self.chatbot_short.message_folder_path = f"./database/{user_id}/"



    # -------------------- äº’å‹•å…¥å£ --------------------
    
    async def handle_interaction(self, role: str, text: str):
        """å¤–éƒ¨ï¼ˆFastAPI routeï¼‰å‘¼å«æ­¤æ–¹æ³•ä¸Ÿè¨Šæ¯é€²ä¾†"""
        # print(f"âš ï¸ æ”¶åˆ°è¨Šæ¯ï¼š{role} - {text}")
        # print(f"âš ï¸ temp_messagesï¼š{self.temp_messages}")
        if not text.strip():
            return

        # æŸ¥è©¢æ¨¡å¼
        if role == "search_user_data":
            return await self.search_similar_statements(     
                query=text,
                top_k=8,
                session=self.db_session
            )

        # åœæ­¢ä¸¦åˆ†æ
        if role == "stop_and_analysis" and len(self.temp_messages) > 1:
            await self.analyze_all()
            return

        # ç¢ºä¿ç¬¬ä¸€å¥ä¸€å®šæ˜¯ user
        if not self.temp_messages and role != "user":
            print("âš ï¸ ç¬¬ä¸€æ¢è¨Šæ¯å¿…é ˆæ˜¯ userï¼Œå·²å¿½ç•¥")
            return

        # åˆä½µåŒè§’è‰²é€£çºŒè¨Šæ¯
        if self.temp_messages and self.temp_messages[-1]["role"] == role:
            self.temp_messages[-1]["content"] += text
            return

        self.temp_messages.append({"role": role, "content": text})
        self.all_messages.append({"role": role, "content": text})

        if len(self.temp_messages) > self.message_analysis_threshold:
            await self.analyze_all()


    async def analyze_all(self):
        """å››ç¨®å¿ƒç†åˆ†æä¸¦è¡ŒåŸ·è¡Œ"""
        print("âš ï¸ é–‹å§‹åˆ†æè¨Šæ¯")
        queries = [
            ("beliefs", *prompt.prompt_message_analysis_beliefs()),
            ("persona", *prompt.prompt_message_analysis_persona()),
            ("emotions", *prompt.prompt_message_analysis_emotions()),
            ("life_story", *prompt.prompt_message_analysis_life_story()),
        ]
        tasks = [self._run_analysis(query, params) for _, query, params in queries]
        await asyncio.gather(*tasks)

    async def _run_analysis(self, query: str, assistant_params: dict):
        print(f"âš ï¸ é–‹å§‹åˆ†æï¼š{query, assistant_params}")
        # 1. å°‡é™¤æœ€å¾Œä¸€æ¢ user ä»¥å¤–çš„ temp_messages äº¤çµ¦ Claude åˆ†æ
        last_user = None
        print(f"âš ï¸ temp_messagesï¼š{self.temp_messages}")
        if self.temp_messages and self.temp_messages[-1]["role"] == "user":
            last_user = self.temp_messages[-1]
            self.temp_messages.pop()
            print(f"âš ï¸ temp_messages[-1]ï¼š{last_user}")

        self.chatbot_short.read_previous_messages(messages_history=self.temp_messages)
        self.temp_messages = [last_user] if last_user else []

        # 2. å‘¼å« LLM
        response = await self.chatbot_short.assistant_with_search_tool(**assistant_params)
        xml_block = ai_chatbot.extract_tagged_content_from_str(response, f"answer_{query}")
        _, df = parse_psychological_analysis(xml_block)

        # 3. å„²å­˜
        await self._append_analysis(df, session=self.db_session)
        await self._batch_embed(batch_size=50, session=self.db_session)

        # 4. æ¸…ç†èŠå¤©æ©Ÿå™¨äººå°è©±
        await self.chatbot_short.save_messages_to_postgresql()
        self.chatbot_short.clear_all_messages()

    
    # ------------ å°å·¥å…·ï¼šç¢ºä¿ session ------------
    @asynccontextmanager
    async def _ensure_session(self, ext=None):
        if ext or self.db_session:
            yield ext or self.db_session
        else:
            async with db.AsyncSessionLocal() as temp:
                yield temp

    # ------------ 1. å¯«å…¥åˆ†æ ------------
    async def _append_analysis(self, df: pd.DataFrame, session=None):
        print(f"âš ï¸ å„²å­˜åˆ†æçµæœï¼š{df}")
        async with self._ensure_session(session) as s:
            await db.insert_analyses(s, self.user_id, df.to_dict("records"))
            await s.commit()

    # ------------ 2. æ‰¹é‡å‘é‡åŒ– ------------
    async def _batch_embed(self, batch_size=50, session=None):
        async with self._ensure_session(session) as s:
            rows = await db.select_unembedded(s, self.user_id)
            print(f"âš ï¸ éœ€è¦å‘é‡åŒ–çš„ç­†æ•¸ï¼š{len(rows)}", rows)
            if not rows:
                return
            for chunk in [rows[i : i + batch_size] for i in range(0, len(rows), batch_size)]:
                texts = [f"{r.statement} || {r.evidence}" for r in chunk]
                resp  = await self.clientGPT.embeddings.create(model=self.embedding_model, input=texts)
                pair  = [(r.id, v.embedding) for r, v in zip(chunk, resp.data)]
                await db.update_vectors(s, pair)
                await s.commit()

    # ------------ 3. ç›¸ä¼¼åº¦æŸ¥è©¢ï¼ˆæ”¹èµ° pgvectorï¼‰------------
    async def search_similar_statements(self, query: str, top_k=5, session=None):
        # å–æŸ¥è©¢å‘é‡
        emb = await self.clientGPT.embeddings.create(model=self.embedding_model, input=query)
        q_vec = emb.data[0].embedding

        # æŸ¥ pgvector
        async with self._ensure_session(session) as s:
            rows = await db.pgvector_search(s, self.user_id, q_vec, top_k)

        # çµ„è£
        results = [
            dict(
                analysis_id=r.id,
                category=r.category,
                subcategory=r.subcategory,
                statement=r.statement,
                evidence=r.evidence,
                score=float(r.distance),
            )
            for r in rows
        ]
        ui = "\n".join(
            f"ğŸ“– {r['statement']}\nğŸ§¾ {r['evidence']}\nğŸ”¹ distance:{r['score']:.3f}\nğŸ“Œ {r['category']}/{r['subcategory']}\n--------"
            for r in results
        )
        return results, ui


# ------------------------------------
# å·¥å…·ï¼šè§£æ XML â†’ DataFrame
# ------------------------------------

def parse_psychological_analysis(xml_input):
    if isinstance(xml_input, list):
        xml_input = xml_input[0]
    xml_string = f"<root>{xml_input}</root>"
    tree = ET.fromstring(xml_string)
    evidence_map = defaultdict(list)
    timestamp = datetime.now().strftime("%Y/%m/%d-%H:%M")

    def _extract(section, cat, sub):
        if section.attrib.get("has_content") == "true":
            for item in section.findall("item"):
                stmt = item.findtext("statement", default="").strip()
                evs = [e.text.strip() for e in item.findall("evidence") if e.text]
                evidence_map[(cat, sub, stmt)].extend(evs)

    for cat_node in tree:
        cat = cat_node.tag
        for sub_node in cat_node:
            _extract(sub_node, cat, sub_node.tag)

    records = [
        {
            "Timestamp": timestamp,
            "Category": k[0],
            "Subcategory": k[1],
            "Statement": k[2],
            "Evidence": " || ".join(v),
        }
        for k, v in evidence_map.items()
    ]
    df = pd.DataFrame(records)
    return records, df
